{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Ring-hydroxylating Dioxygenases"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lucas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/lucas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/lucas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/lucas/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import re\n",
    "from prince import CA, MCA\n",
    "import matplotlib.cm as cm\n",
    "from Bio.SeqUtils import seq3\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import OPTICS\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "from sklearn.manifold import MDS\n",
    "from scipy.cluster.hierarchy import to_tree\n",
    "from operator import itemgetter\n",
    "import fastcluster\n",
    "import scipy.cluster.hierarchy as hierarchy\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Download necessary resources from NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-22T16:06:40.060517Z",
     "start_time": "2023-06-22T16:06:40.057141Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# # # EXAMPLE of how to merge data frames (alignment + metadata)\n",
    "# # Join the dataframes based on the 'ID' column\n",
    "# if len(df.columns) == len(col_idx):\n",
    "#     if (df.columns == col_idx).all():\n",
    "#         df = pd.merge(\n",
    "#             pd.merge(\n",
    "#                 pd.read_csv('data.tsv', delimiter='\\t'),\n",
    "#                 pd.DataFrame(\n",
    "#                     {\n",
    "#                         'Entry':[headers[idx].split('/')[0].split('_')[0] for idx in df.index],\n",
    "#                         'Index':list(df.index)\n",
    "#                     }\n",
    "#                 ),\n",
    "#                 on='Entry'\n",
    "#             ),\n",
    "#             df,\n",
    "#             left_on='Index',\n",
    "#             right_index=True,\n",
    "#             how='inner'\n",
    "#         ).copy()\n",
    "# print(df.info)\n",
    "\n",
    "def generate_wordcloud(df):\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Extract the substrate and enzyme names using regular expressions\n",
    "    matches = df['Protein names'].str.extract(r'(.+?) ([\\w\\-,]+ase)', flags=re.IGNORECASE)\n",
    "    # String normalization pipeline\n",
    "    df['Substrate'] = matches[0] \\\n",
    "        .fillna('') \\\n",
    "        .apply(lambda x: '/'.join(re.findall(r'\\b(\\w+(?:ene|ine|ate|yl))\\b', x, flags=re.IGNORECASE))) \\\n",
    "        .apply(lambda x: x.lower())\n",
    "    df['Enzyme'] = matches[1] \\\n",
    "        .fillna('') \\\n",
    "        .apply(lambda x: x.split('-')[-1] if '-' in x else x) \\\n",
    "        .apply(lambda x: x.lower())\n",
    "    df['Label'] = df['Substrate'].str.cat(df['Enzyme'], sep=' ').str.strip()\n",
    "    df = df.copy()\n",
    "    # Plot the word cloud\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(\n",
    "        WordCloud(\n",
    "            width=800,\n",
    "            height=400,\n",
    "            background_color='white'\n",
    "        ).generate(\n",
    "            ' '.join(\n",
    "                sorted(\n",
    "                    set([string for string in df.Label.values.tolist() if len(string) > 0])\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "        interpolation='bilinear'\n",
    "    )\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "def parse_msa_file(msa_file, msa_format=\"fasta\"):\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    headers, sequences = [], []\n",
    "    for record in SeqIO.parse(msa_file, msa_format).records:\n",
    "        headers.append(record.id)\n",
    "        sequences.append(record.seq)\n",
    "    seq_array = np.array(sequences)\n",
    "    return seq_array, headers\n",
    "\n",
    "def clean_msa(df, threshold=.9, plot=False):\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df.replace(\n",
    "        ['-', *[chr(i) for i in range(ord('a'), ord('z')+1)]],\n",
    "        np.nan,\n",
    "        inplace=True\n",
    "    )\n",
    "    min_rows = int(threshold * df.shape[0])\n",
    "    # Remove columns with NaN values above the threshold\n",
    "    df.dropna(thresh=min_rows, axis=1, inplace=True)\n",
    "    min_cols = int(threshold * df.shape[1])\n",
    "    # Remove rows with NaN values above the threshold\n",
    "    df.dropna(thresh=min_cols, axis=0, inplace=True)\n",
    "    if plot:\n",
    "        # Plot the heatmap\n",
    "        sns.heatmap(df.isna().astype(int), cmap='binary', xticklabels=False, yticklabels=False, cbar=False)\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "    return df\n",
    "\n",
    "def shrink(data):\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Perform MCA\n",
    "    mca = MCA()\n",
    "    mca.fit(data)\n",
    "    return mca\n",
    "\n",
    "def get_clusters(mca_object, plot=False):\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Access the results\n",
    "    coordinates = mca_object.transform(data)\n",
    "    # Convert the coordinates to a NumPy array\n",
    "    coordinates = np.array(coordinates)\n",
    "    # Define a range of potential number of clusters to evaluate\n",
    "    min_clusters = 3\n",
    "    max_clusters = 10\n",
    "    # Perform clustering for different number of clusters and compute silhouette scores\n",
    "    silhouette_scores = []\n",
    "    for k in range(min_clusters, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, n_init=10)  # Set n_init explicitly\n",
    "        kmeans.fit(coordinates)\n",
    "        labels = kmeans.labels_\n",
    "        score = silhouette_score(coordinates, labels)\n",
    "        silhouette_scores.append(score)\n",
    "    # Find the best number of clusters based on the highest silhouette score\n",
    "    best_num_clusters = np.argmax(silhouette_scores) + min_clusters\n",
    "    # Perform clustering with the best number of clusters\n",
    "    kmeans = KMeans(n_clusters=best_num_clusters, n_init=10)  # Set n_init explicitly\n",
    "    kmeans.fit(coordinates)\n",
    "    kmeans_cluster_labels = kmeans.labels_\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    if plot:\n",
    "        if len(data) <= 5000:\n",
    "            # Plot together the scatter plot of both sequences and residues overlaid\n",
    "            mca.plot(\n",
    "                data,\n",
    "                x_component=0,\n",
    "                y_component=1\n",
    "            )\n",
    "        # Plot the scatter plot colored by clusters\n",
    "        plt.scatter(coordinates[:, 0], coordinates[:, 1], c=kmeans_cluster_labels, cmap='viridis')\n",
    "        plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', marker='x', label='Cluster Centers')\n",
    "        plt.xlabel('Dimension 1')\n",
    "        plt.ylabel('Dimension 2')\n",
    "        plt.title('Scatter Plot - Clusters')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return kmeans_cluster_labels\n",
    "\n",
    "def henikoff(data):\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_array = data.to_numpy()  # Convert DataFrame to NumPy array\n",
    "    size, length = data_array.shape\n",
    "    weights = []\n",
    "    for seq_index in range(size):\n",
    "        row = data_array[seq_index, :]\n",
    "        unique_vals, counts = np.unique(row, return_counts=True)\n",
    "        k = len(unique_vals)\n",
    "        matrix_row = 1. / (k * counts)\n",
    "        weights.append(np.sum(matrix_row) / length)\n",
    "    return pd.Series(weights, index=data.index)\n",
    "\n",
    "def select_features(data, labels, plot=False):\n",
    "    X = data\n",
    "    y = pd.get_dummies(labels).astype(int) if len(\n",
    "        set(labels)\n",
    "    ) > 2 else labels\n",
    "    # Perform one-hot encoding on the categorical features\n",
    "    encoder = OneHotEncoder()\n",
    "    X_encoded = encoder.fit_transform(X)\n",
    "    # Get the column names for the encoded features\n",
    "    encoded_feature_names = []\n",
    "    for i, column in enumerate(X.columns):\n",
    "        categories = encoder.categories_[i]\n",
    "        for category in categories:\n",
    "            feature_name = f'{column}_{category}'\n",
    "            encoded_feature_names.append(feature_name)\n",
    "    # Convert X_encoded to DataFrame\n",
    "    X_encoded_df = pd.DataFrame.sparse.from_spmatrix(X_encoded, columns=encoded_feature_names)\n",
    "    # Create and train the Random Forest classifier\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_encoded_df, y)\n",
    "    # Feature selection\n",
    "    feature_selector = SelectFromModel(rf, threshold='median')\n",
    "    feature_selector.fit_transform(X_encoded_df, y)\n",
    "    selected_feature_indices = feature_selector.get_support(indices=True)\n",
    "    selected_features = X_encoded_df.columns[selected_feature_indices]\n",
    "    # Calculate feature importances for original columns\n",
    "    sorted_importance = pd.DataFrame(\n",
    "        {\n",
    "            'Residues': selected_features,\n",
    "            'Importance': rf.feature_importances_[selected_feature_indices],\n",
    "            'Columns': map(lambda x: int(x.split('_')[0]), selected_features)\n",
    "        }\n",
    "    )[['Columns', 'Importance']].groupby('Columns').sum()['Importance'].sort_values(ascending=False)\n",
    "    sorted_features = sorted_importance.index\n",
    "    if plot:\n",
    "        fig, ax1 = plt.subplots(figsize=(16, 4))\n",
    "        # Bar chart of percentage importance\n",
    "        xvalues = range(len(sorted_features))\n",
    "        ax1.bar(xvalues, sorted_importance, color='b')\n",
    "        ax1.set_ylabel('Summed Importance')\n",
    "        ax1.tick_params(axis='y')\n",
    "        # Line chart of cumulative percentage importance\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(xvalues, np.cumsum(sorted_importance) / np.sum(sorted_importance), color='r', marker='.')\n",
    "        ax2.set_ylabel('Cumulative Importance')\n",
    "        ax2.tick_params(axis='y')\n",
    "        # Rotate x-axis labels\n",
    "        plt.xticks(xvalues, sorted_features)\n",
    "        plt.setp(ax1.xaxis.get_majorticklabels(), rotation=90)\n",
    "        plt.setp(ax2.xaxis.get_majorticklabels(), rotation=90)\n",
    "        plt.show()\n",
    "    return selected_features, sorted_importance\n",
    "\n",
    "def get_labels(data, plot=False):\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Perform MCA\n",
    "    mca_object = shrink(data)\n",
    "    # Perform feature selection on data\n",
    "    selected_features, sorted_importance = select_features(data, labels=get_clusters(mca_object))\n",
    "    # Calculate cumulative sum of importance\n",
    "    cumulative_importance = np.cumsum(sorted_importance) / np.sum(sorted_importance)\n",
    "    # Find the index where cumulative importance exceeds or equals 0.9\n",
    "    index = np.where(cumulative_importance >= 0.9)[0][0]\n",
    "    # Get the values from sorted_features up to the index\n",
    "    selected_columns = sorted_importance.index[:index+1].values\n",
    "    # Filter the selected features to get most important residues\n",
    "    selected_residues = [x for x in selected_features if int(x.split('_')[0]) in selected_columns]\n",
    "    df_res = mca_object.column_coordinates(data[selected_columns]).loc[selected_residues]\n",
    "    # Get sequence weights through Henikoff & Henikoff algorithm\n",
    "    weights = henikoff(data[selected_columns])\n",
    "    # Create an empty graph\n",
    "    G = nx.Graph()\n",
    "    for idx in df_res.index:\n",
    "        col, aa = idx.split('_')\n",
    "        col = int(col)\n",
    "        rows = data[selected_columns].index[data[selected_columns][col] == aa].tolist()\n",
    "        # Filter and sum values based on valid indices\n",
    "        p = weights.iloc[[i for i in rows if i < len(weights)]].sum()\n",
    "        # Add a node with attributes\n",
    "        G.add_node(\n",
    "            f'{seq3(aa)}{col}',\n",
    "            idx=idx,\n",
    "            aa=aa,\n",
    "            col=col,\n",
    "            coord=(\n",
    "                df_res.loc[idx,0],\n",
    "                df_res.loc[idx,1]\n",
    "            ),\n",
    "            rows=rows,\n",
    "            p=p\n",
    "        )\n",
    "    nodelist = sorted(G.nodes(), key=lambda x: G.nodes[x]['p'], reverse=True)\n",
    "    df_res = df_res.loc[[G.nodes[u]['idx'] for u in nodelist]]\n",
    "    df_res.columns = ['x_mca', 'y_mca']\n",
    "    df_res = df_res.copy()\n",
    "    # Generate pairwise combinations\n",
    "    pairwise_comparisons = list(combinations(G.nodes, 2))\n",
    "    # Add edges to graph based on pairwise calculation of Jaccard's dissimilarity (1 - similarity)\n",
    "    for u, v in pairwise_comparisons:\n",
    "        asymmetric_distance = set(G.nodes[u]['rows']) ^ set(G.nodes[v]['rows'])\n",
    "        union = set(G.nodes[u]['rows']) | set(G.nodes[v]['rows'])\n",
    "        weight = float(\n",
    "            weights.iloc[[i for i in list(asymmetric_distance) if i < len(weights)]].sum()\n",
    "        ) / float(\n",
    "            weights.iloc[[i for i in list(union) if i < len(weights)]].sum()\n",
    "        ) if G.nodes[u]['col'] != G.nodes[v]['col'] else 1.\n",
    "        G.add_edge(\n",
    "            u,\n",
    "            v,\n",
    "            weight = weight\n",
    "        )\n",
    "    # Generate distance matrix\n",
    "    D = nx.to_numpy_array(G, nodelist=nodelist)\n",
    "    # Apply OPTICS on the points\n",
    "    optics = OPTICS(metric='precomputed', min_samples=3)\n",
    "    optics.fit(D)\n",
    "    # Retrieve cluster labels\n",
    "    cluster_labels = optics.labels_\n",
    "    # Find the unique values and their counts\n",
    "    unique_values, counts = np.unique(cluster_labels, return_counts=True)\n",
    "    # Calculate the proportion of unique values\n",
    "    proportions = counts / len(cluster_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # Need refactoring so clusters contain residue objects instead of just indices\n",
    "    # unique_labels = np.unique(cluster_labels)\n",
    "    # clusters = [[] for _ in unique_labels]\n",
    "    # for i, cluster_label in enumerate(unique_labels):\n",
    "    #     clusters[i] = np.where(cluster_labels == cluster_label)[0].tolist()\n",
    "    # adhesion = []\n",
    "    # for row_idx in data.index:\n",
    "    #     temp = []\n",
    "    #     for cluster in clusters:\n",
    "    #         count = 0\n",
    "    #         for cluster_idx in cluster:\n",
    "    #             node_idx = nodelist[cluster_idx]\n",
    "    #             if data.loc[row_idx, G.nodes[node_idx]['col']] == G.nodes[node_idx]['aa']:\n",
    "    #                 count += 1\n",
    "    #         temp.append(float(count)/float(len(cluster)))\n",
    "    #     adhesion.append(np.array(temp))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    adhesion = np.array(adhesion)\n",
    "    df_adh = pd.DataFrame(adhesion)\n",
    "    df_adh.columns = [f\"Cluster {label+1}\" if label >= 0 else \"Noise\" for label in unique_labels]\n",
    "    itemgetter_func = itemgetter(*data.index)\n",
    "    seq_ids = itemgetter_func(headers)\n",
    "    df_adh.index = seq_ids\n",
    "    # Run clustermap with potential performance improvement\n",
    "    Z = fastcluster.linkage(df_adh, method='ward')\n",
    "    # List to store silhouette scores\n",
    "    silhouette_scores = []\n",
    "    # Define a range of possible values for k\n",
    "    k_values = range(2, 10)\n",
    "    # Calculate silhouette score for each value of k\n",
    "    for k in k_values:\n",
    "        labels = fcluster(Z, k, criterion='maxclust')\n",
    "        silhouette_scores.append(silhouette_score(df_adh, labels))\n",
    "    # Find the index of the maximum silhouette score\n",
    "    best_index = np.argmax(silhouette_scores)\n",
    "    # Get the best value of k\n",
    "    best_k = k_values[best_index]\n",
    "    # Get the cluster labels for each sequence in the MSA\n",
    "    sequence_labels = fcluster(Z, best_k, criterion='maxclust')\n",
    "    if plot:\n",
    "        # Plot the distance matrix\n",
    "        fig, ax = plt.subplots()\n",
    "        im = ax.imshow(D, cmap='viridis')\n",
    "        # Add a colorbar\n",
    "        cbar = ax.figure.colorbar(im, ax=ax)\n",
    "        # Retrieve ordering information\n",
    "        ordering = optics.ordering_\n",
    "        # Perform MDS to obtain the actual points\n",
    "        mds = MDS(n_components=2, dissimilarity='precomputed', normalized_stress='auto')\n",
    "        points = mds.fit_transform(D)\n",
    "\n",
    "        df_res['x_mds'] = points[:,0]\n",
    "        df_res['y_mds'] = points[:,1]\n",
    "        df_res['label'] = cluster_labels\n",
    "        df_res = df_res.copy()\n",
    "        # Plot the ordering analysis\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.bar(range(len(ordering)), ordering, width=1., color='black')\n",
    "        plt.xlim([0, len(ordering)])\n",
    "        plt.xlabel('Points')\n",
    "        plt.ylabel('Reachability Distance')\n",
    "        plt.title('Ordering Analysis')\n",
    "        # Show the plots\n",
    "        plt.show()\n",
    "        # Create a figure with two subplots\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 7))\n",
    "        # Plot for MCA\n",
    "        axs[0].set_xlabel('MCA Dimension 1', fontsize=14)\n",
    "        axs[0].set_ylabel('MCA Dimension 2', fontsize=14)\n",
    "        axs[0].set_title('Residues from MCA Colored by Cluster', fontsize=16)\n",
    "        # Plot for MDS\n",
    "        axs[1].set_xlabel('MDS Dimension 1', fontsize=14)\n",
    "        axs[1].set_ylabel('MDS Dimension 2', fontsize=14)\n",
    "        axs[1].set_title('Residues from MDS Colored by Cluster', fontsize=16)\n",
    "        x_mca = df_res['x_mca']\n",
    "        y_mca = df_res['y_mca']\n",
    "        x_mds = df_res['x_mds']\n",
    "        y_mds = df_res['y_mds']\n",
    "        labels = df_res['label']\n",
    "        unique_labels = np.unique(labels)\n",
    "        color_map = cm.get_cmap('tab10', len(unique_labels))\n",
    "        handles = []\n",
    "        all_labels = []\n",
    "        for i, cluster_label in enumerate(unique_labels):\n",
    "            if cluster_label == -1:\n",
    "                noise_color = 'grey'\n",
    "                noise_marker = 'x'\n",
    "                label = 'Noise'\n",
    "            else:\n",
    "                noise_color = color_map(i)\n",
    "                noise_marker = 'o'\n",
    "                label = f'Cluster {cluster_label+1}'\n",
    "            scatter_mca = axs[0].scatter(x_mca[labels == cluster_label], y_mca[labels == cluster_label], color=noise_color, marker=noise_marker, label=label)\n",
    "            scatter_mds = axs[1].scatter(x_mds[labels == cluster_label], y_mds[labels == cluster_label], color=noise_color, marker=noise_marker, label=label)\n",
    "            # Collect the scatter plot handles and labels\n",
    "            handles.append(scatter_mca)\n",
    "            all_labels.append(label)\n",
    "        # Rearrange labels to fill one row first and then the second row\n",
    "        n_cols = 6  # Number of columns in the legend\n",
    "        all_labels_reordered = [all_labels[i::n_cols] for i in range(n_cols)]\n",
    "        all_labels_reordered = sum(all_labels_reordered, [])  # Flatten the nested list\n",
    "        # Create a single legend for both subplots with reordered labels\n",
    "        fig.legend(handles, all_labels_reordered, bbox_to_anchor=(0.5, 0.0), loc='upper center', borderaxespad=0, ncol=n_cols, fontsize=12)\n",
    "        # Adjust the layout to accommodate the legend\n",
    "        fig.tight_layout(rect=[0, 0.05, 1, 0.9])\n",
    "        cbar_kws = {\"orientation\": \"horizontal\"}\n",
    "        g = sns.clustermap(df_adh.drop('Noise', axis=1), col_cluster=False, yticklabels=False, cbar_pos=(.4, .9, .4, .02), cbar_kws=cbar_kws, figsize=(5, 5))\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "    return cluster_labels, sequence_labels\n",
    "\n",
    "\n",
    "def refine():\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return None\n",
    "\n",
    "def bootstrap():\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return None\n",
    "\n",
    "def get_newick(node, parent_dist, leaf_names, newick=\"\"):\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if node.is_leaf():\n",
    "        return f\"{leaf_names[node.id]}:{parent_dist - node.dist:.2f}{newick}\"\n",
    "    else:\n",
    "        if len(newick) > 0:\n",
    "            newick = f\"):{parent_dist - node.dist:.2f}{newick}\"\n",
    "        else:\n",
    "            newick = \");\"\n",
    "        newick = get_newick(node.get_left(), node.dist, leaf_names, newick)\n",
    "        newick = get_newick(node.get_right(), node.dist, leaf_names, f\",{newick}\")\n",
    "        newick = f\"({newick}\"\n",
    "        return newick\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create data frame from metadata and generate word clouds\n",
    "generate_wordcloud(pd.read_csv('data.tsv', delimiter='\\t'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8442 36\n",
      "<bound method DataFrame.info of      182 185 186 187 200 204 205 206 216 217  ... 838 841 845 848 851 853 982   \n",
      "0      A   L   Y   C   E   G   F   H   V   H  ...   T   Y   V   Y   D   E   E  \\\n",
      "1      K   L   Y   M   D   P   Y   H   L   H  ...   Y   F   G   Y   E   D   D   \n",
      "2      K   L   T   Y   E   N   Y   H   I   H  ...   L   Y   W   K   G   E   E   \n",
      "3      K   V   F   C   D   G   Y   H   A   H  ...   Y   F   L   E   G   S   E   \n",
      "4      K   V   V   N   E   C   Y   H   N   H  ...   W   F   V   A   D   A   Q   \n",
      "...   ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..  ..   \n",
      "9373   K   T   F   I   E   D   Y   H   F   H  ...   F   F   Y   P   E   E   E   \n",
      "9376   K   M   Y   L   D   T   V   H   T   H  ...   S   F   R   A   A   G   D   \n",
      "9377   K   L   T   M   E   C   Y   H   N   H  ...   W   L   V   H   R   D   Q   \n",
      "9378   K   L   V   V   D   F   Y   H   V   H  ...   V   F   V   D   N   S   Q   \n",
      "9380   K   L   T   M   E   C   Y   H   N   H  ...   W   C   V   H   R   D   Q   \n",
      "\n",
      "     983 984 987  \n",
      "0      D   E   F  \n",
      "1      D   T   E  \n",
      "2      D   R   D  \n",
      "3      D   I   E  \n",
      "4      D   K   E  \n",
      "...   ..  ..  ..  \n",
      "9373   D   D   E  \n",
      "9376   D   V   H  \n",
      "9377   D   R   A  \n",
      "9378   D   V   P  \n",
      "9380   D   R   R  \n",
      "\n",
      "[7568 rows x 41 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Create data frame from raw data and clean it\n",
    "raw, headers = parse_msa_file('alignment.fasta')\n",
    "msa = pd.DataFrame(raw)\n",
    "df = clean_msa(msa)\n",
    "# Backup original indices for further usage\n",
    "row_idx, col_idx = df.index, df.columns\n",
    "print(df.info)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-22T15:36:43.046751Z",
     "start_time": "2023-06-22T15:36:32.572527Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method DataFrame.info of      182 185 186 187 200 204 205 206 216 217  ... 838 841 845 848 851 853 982   \n0      K   L   L   I   E   C   Y   H   S   H  ...   V   Y   R   H   R   D   E  \\\n1      K   S   I   V   E   C   Y   H   A   H  ...   I   Y   F   L   N   K   E   \n2      K   N   I   V   E   C   Y   H   A   H  ...   I   Y   F   T   N   E   E   \n3      K   T   L   A   E   C   Y   H   A   H  ...   V   Y   R   H   V   D   E   \n4      K   I   I   V   E   C   Y   H   A   H  ...   I   Y   F   R   N   K   E   \n...   ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..  ..   \n7218   K   M   V   V   D   G   Y   H   T   H  ...   A   V   L   I   P   G   D   \n7221   K   L   Y   M   E   F   Y   H   L   H  ...   L   Y   F   P   A   P   Q   \n7223   K   M   V   V   D   G   Y   H   T   H  ...   A   V   L   M   P   G   D   \n7224   K   V   L   V   E   G   Y   H   T   H  ...   S   M   V   T   P   M   -   \n7225   K   T   F   I   E   D   Y   H   F   H  ...   F   Y   Y   P   E   D   E   \n\n     983 984 987  \n0      D   R   D  \n1      D   L   R  \n2      D   L   R  \n3      D   K   H  \n4      D   L   R  \n...   ..  ..  ..  \n7218   D   G   E  \n7221   D   A   N  \n7223   D   G   E  \n7224   -   -   -  \n7225   D   K   E  \n\n[6219 rows x 41 columns]>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the main data set to be used along the data flow\n",
    "data = df[col_idx].drop_duplicates().fillna('-').copy()\n",
    "print(data.info)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-22T15:36:43.853368Z",
     "start_time": "2023-06-22T15:36:43.774808Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-22T18:23:35.801184Z",
     "start_time": "2023-06-22T18:23:35.561978Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 8\n",
      "Best silhouette score: 0.4250574761267073\n",
      "[1 2 3 4 5 6 7 8]\n"
     ]
    }
   ],
   "source": [
    "cluster_labels, sequence_labels = get_labels(data)\n",
    "\n",
    "# Print the cluster labels\n",
    "print(np.unique(sequence_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-22T15:37:38.187873Z",
     "start_time": "2023-06-22T15:37:34.514522Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "array([8, 8, 8, ..., 3, 8, 7], dtype=int32)"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-22T16:15:18.972723Z",
     "start_time": "2023-06-22T16:15:18.970002Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
