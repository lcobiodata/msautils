{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DB[RC/S3]\n",
    "Density-Based Residue Clustering by Dissimilarity Between Sequence SubSets)\n",
    "#\n",
    "## Ring-hydroxylating Dioxygenases"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from protlearn import *\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###====================================================================================================\n",
    "### Parameters\n",
    "###====================================================================================================\n",
    "\n",
    "class Args(object):\n",
    "    def __init__(self) -> None:\n",
    "        self.__getattr__ = None\n",
    "args = Args()\n",
    "args.file = 'alignment.fasta'\n",
    "args.expand_alphabet = False\n",
    "args.min_freq = .25\n",
    "args.max_dist = 1.\n",
    "args.min_size = 3\n",
    "args.out = None\n",
    "\n",
    "if args.out is None:\n",
    "    args.out = 'output/' + args.file.split('.')[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #====================================================================================================\n",
    "msa = MSA()\n",
    "msa.parse(args.file)\n",
    "msa.read()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "msa.headers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "msa.sequences.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "msa.weights = [1./ float(msa.size) for i in range(msa.size)]\n",
    "# msa.henikoff()\n",
    "msa.weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_msa = pd.DataFrame(msa.sequences)\n",
    "\n",
    "# Calculate the proportion of \"-\" values weighted by row weight\n",
    "gap_ratio = (df_msa == '-').mul(msa.weights, axis=0).sum()\n",
    "\n",
    "# Filter the columns based on the condition that \"-\" is present in more than 90% of the rows\n",
    "selected_columns = gap_ratio.index[(gap_ratio < args.min_freq)]\n",
    "\n",
    "# Select only the columns corresponding to the selected features\n",
    "df_raw = df_msa[selected_columns]\n",
    "df_raw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Attention!!! This snippet is specific for this particular example so it must be adapted to\n",
    "each particular case.\n",
    "\"\"\"\n",
    "df_metadata = pd.read_csv('data.tsv', delimiter='\\t').dropna(subset='EC number')\n",
    "\n",
    "df_metadata['EC number'] = df_metadata['EC number'].apply(lambda x: x.split('; '))\n",
    "df_metadata"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seq_ids = list(map(lambda x: x.split('/')[0].split('_')[0], msa.headers))\n",
    "seq_idx = list(range(msa.size))\n",
    "df_indices = pd.DataFrame({'Entry':seq_ids, 'Index':seq_idx})\n",
    "df_indices"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Join the dataframes based on the 'ID' column\n",
    "df_indexed = pd.merge(df_metadata, df_indices, on='Entry')\n",
    "df_merged = pd.merge(df_indexed, df_raw, left_on='Index', right_index=True, how='inner')\n",
    "df_merged"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_label = df_merged.explode('EC number')\n",
    "df_label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_label['EC number'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_label['Label'] = df_label['EC number'].apply(lambda x: '.'.join(x.split('.')[:-1]))\n",
    "df_label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_label['Label'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_encoded = pd.get_dummies(df_label['Label']).astype(int)\n",
    "df_encoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_chars = df_label[df_raw.columns]\n",
    "df_chars"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def target_mean(df, by, on):\n",
    "    means = df.groupby(by)[on].mean()\n",
    "    return df[by].map(means)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ftr_imp = []\n",
    "for i in df_encoded.columns:\n",
    "    target = df_encoded[i].tolist()\n",
    "    df_num = df_chars.copy()\n",
    "    df_num['Target'] = target\n",
    "    for j in df_num.columns:\n",
    "        df_num[j] = target_mean(df_num, by=j, on='Target')\n",
    "\n",
    "    # Split the dataset into features (X) and target (y)\n",
    "    X = df_num.drop('Target', axis=1)\n",
    "    y = df_num['Target']\n",
    "\n",
    "    # Fit a random forest model to the data\n",
    "    rf = RandomForestRegressor(random_state=0)\n",
    "    rf.fit(X, y)\n",
    "    ftr_imp.append(rf.feature_importances_)\n",
    "arr = np.array(ftr_imp)\n",
    "averages = np.mean(arr, axis=0)\n",
    "\n",
    "averages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the feature importances and sort them in descending order\n",
    "importances = pd.Series(averages, index=X.columns).sort_values(ascending=False)\n",
    "importances"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate the cumulative sum of the importance values\n",
    "cumulative_importance = importances.cumsum()\n",
    "cumulative_importance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filter the feature importances to keep only those that contribute to 99% of the importance\n",
    "most_important = importances[cumulative_importance <= 0.75].sort_values(ascending=False)\n",
    "selected_features = importances[cumulative_importance <= 0.75].index\n",
    "higher_importance = cumulative_importance[selected_features]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(16, 4))\n",
    "\n",
    "# Bar chart of percentage importance\n",
    "xvalues = range(len(most_important))\n",
    "ax1.bar(xvalues, most_important, color='b')\n",
    "ax1.set_ylabel('Percentage of total importance')\n",
    "ax1.tick_params(axis='y')\n",
    "\n",
    "# Line chart of cumulative percentage importance\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(xvalues, higher_importance, color='r', marker='.')\n",
    "ax2.set_ylabel('Cumulative importance')\n",
    "ax2.tick_params(axis='y')\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(xvalues, most_important.index)\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=90)\n",
    "plt.setp(ax2.xaxis.get_majorticklabels(), rotation=90)\n",
    "\n",
    "# Adjust layout to make sure labels are visible\n",
    "# plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select only the columns corresponding to the selected features\n",
    "df_selected = df_num[selected_features]\n",
    "df_selected"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# _, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "#\n",
    "# # Plot the original DataFrame\n",
    "# sns.heatmap(X, cmap='coolwarm', xticklabels=False, yticklabels=False, ax=axs[0])\n",
    "# axs[0].set_title('Original DataFrame')\n",
    "#\n",
    "# # Plot the sorted DataFrame\n",
    "# sns.heatmap(X[importances.index], cmap='coolwarm', xticklabels=False, yticklabels=False, ax=axs[1])\n",
    "# axs[1].set_title('Sorted DataFrame by\\ndescending feature importance')\n",
    "#\n",
    "# # Plot the filtered DataFrame\n",
    "# sns.heatmap(df_selected, cmap='coolwarm', yticklabels=False, ax=axs[2])\n",
    "# axs[2].set_title('Filtered DataFrame by\\ncumulative feature importance')\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #====================================================================================================\n",
    "R = []\n",
    "for col in most_important.index:\n",
    "    R += msa.collection[col]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #====================================================================================================\n",
    "G = nx.Graph()\n",
    "for i, a in enumerate(R[:-1]):\n",
    "    if a.p() >= args.min_freq:\n",
    "        for b in R[i + 1:]:\n",
    "            if b.p() >= args.min_freq:\n",
    "                G.add_edge(\n",
    "                    a,\n",
    "                    b,\n",
    "                    weight = float(\n",
    "                        sum(\n",
    "                            map(lambda x: msa.weights[x], a.sequence_indices ^ b.sequence_indices)\n",
    "                        )\n",
    "                    ) / float(\n",
    "                        sum(\n",
    "                            map(lambda x: msa.weights[x], a.sequence_indices | b.sequence_indices)\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "# #====================================================================================================\n",
    "N = sorted(G.nodes(), key=lambda x: x.p(), reverse=True)\n",
    "for n in N:\n",
    "    print(n)\n",
    "# #====================================================================================================\n",
    "D = nx.to_numpy_array(G, nodelist=N)\n",
    "D"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the distance matrix\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(D, cmap='viridis')\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #====================================================================================================\n",
    "optics_instance = optics(D, args.max_dist, args.min_size, None, 'distance_matrix')\n",
    "optics_instance.process()\n",
    "clusters = optics_instance.get_clusters()\n",
    "# #====================================================================================================\n",
    "ordering = ordering_analyser(optics_instance.get_ordering())\n",
    "ordering = ordering.cluster_ordering\n",
    "plt.figure()\n",
    "plt.bar(range(0, len(ordering)), ordering[0:len(ordering)], width=1., color='black')\n",
    "plt.xlim([0, len(ordering)])\n",
    "plt.xlabel('Points')\n",
    "plt.ylabel('Reachability Distance')\n",
    "plt.savefig('%s_reachability_plot.png' % args.out)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #====================================================================================================\n",
    "clusters = sorted(clusters, key=lambda x: np.mean(list(map(lambda y: N[y].p(), x))), reverse=True)\n",
    "i = 0\n",
    "while i < len(clusters):\n",
    "    positions = set(map(lambda x: N[x].position, clusters[i]))\n",
    "    same_position = {k: [] for k in positions}\n",
    "    for j in clusters[i]:\n",
    "        same_position[N[j].position].append(j)\n",
    "    temp = []\n",
    "    c = Subset(msa, list(set.union(*map(lambda x: set(N[x].sequence_indices), clusters[i]))))\n",
    "    for j in clusters[i]:\n",
    "        if j == max(same_position[N[j].position], key=lambda x: N[x].p.given(c)):\n",
    "            temp.append(j)\n",
    "    if len(temp) >= args.min_size:\n",
    "        clusters[i] = temp\n",
    "        i += 1\n",
    "    else:\n",
    "        del clusters[i]\n",
    "clusters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #====================================================================================================\n",
    "with open('%s_clusters.csv' % args.out, 'w') as outfile:\n",
    "    for i in range(len(clusters)):\n",
    "        outfile.write('Cluster %d\\n' % (i + 1))\n",
    "        d = {'MSA\\nColumn': [], 'Feature': [], 'Frequency': []}\n",
    "        for j in sorted(clusters[i], key=lambda x: N[x].position):\n",
    "            d['MSA\\nColumn'].append(N[j].position + 1)\n",
    "            d['Feature'].append(N[j])\n",
    "            d['Frequency'].append('%.2f' % round(N[j].p(), 2))\n",
    "        df = pd.DataFrame(d)\n",
    "        outfile.write(df.to_csv(index=False))\n",
    "        outfile.write('\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #====================================================================================================\n",
    "H = []\n",
    "for i in range(msa.size):\n",
    "    row = []\n",
    "    for j in range(len(clusters)):\n",
    "        count = 0\n",
    "        for k in clusters[j]:\n",
    "            if i in N[k].sequence_indices:\n",
    "                count += 1\n",
    "        row.append(float(count) / float(len(clusters[j])))\n",
    "    H.append(row)\n",
    "H = np.array(H)\n",
    "H"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #====================================================================================================\n",
    "Z = linkage(H, 'average')\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "dn = dendrogram(Z, labels=np.array(msa.headers))\n",
    "plt.savefig('%s_dendrogram.png' % args.out)\n",
    "tree = to_tree(Z, False)\n",
    "with open('%s_dendrogram.nwk' % args.out, 'w') as outfile:\n",
    "    outfile.write(get_newick(tree, \"\", tree.dist, msa.headers))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #====================================================================================================\n",
    "df = get_df(H, msa, range(msa.size), range(len(clusters)))\n",
    "seq = df.pop('Seq. ID')\n",
    "try:\n",
    "    g = sns.clustermap(df, col_cluster=False, yticklabels=False, figsize=(4,4))\n",
    "except SystemExit:\n",
    "    raise 'Warning: few clusters to draw a heatmap!'\n",
    "row_idx = g.dendrogram_row.reordered_ind\n",
    "# col_idx = g.dendrogram_col.reordered_ind\n",
    "col_idx = range(len(clusters))  # Keep column index without dendrogram\n",
    "H = [H[i] for i in row_idx]\n",
    "H = np.array(H)\n",
    "df = get_df(H, msa, row_idx, col_idx)\n",
    "df.to_csv('%s_seq_adhesion.csv' % args.out)\n",
    "plt.savefig('%s_seq_adhesion.png' % args.out)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #====================================================================================================\n",
    "# # Optional viewing\n",
    "# #====================================================================================================\n",
    "mds = manifold.MDS(n_components=2, dissimilarity=\"precomputed\", normalized_stress='auto')\n",
    "pts = mds.fit(D).embedding_\n",
    "clf = PCA(n_components=2)\n",
    "pts = clf.fit_transform(pts)\n",
    "# #====================================================================================================\n",
    "colors = np.array(list(map(lambda x: x.p(), N))) * 100\n",
    "_, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot 1: Residue Plot with noise\n",
    "X_full, Y_full = zip(*pts)\n",
    "sc = axs[0].scatter(X_full, Y_full, c=colors, cmap='rainbow', vmin=0., vmax=100., alpha=.5)\n",
    "cb = plt.colorbar(sc, ax=axs[0])\n",
    "cb.set_label('Frequency (%s)' % '%')\n",
    "axs[0].set_title('Residue Plot with noise')\n",
    "\n",
    "# #====================================================================================================\n",
    "# Plot 2: Residue Plot without noise\n",
    "noise = optics_instance.get_noise()\n",
    "points, colors = [], []\n",
    "for i, (p, c) in enumerate(zip(pts, list(map(lambda x: x.p(), N)))):\n",
    "    if i not in noise:\n",
    "        points.append(p)\n",
    "        colors.append(c)\n",
    "colors = np.array(colors) * 100\n",
    "X_clean, Y_clean = zip(*points)\n",
    "sc = axs[1].scatter(X_clean, Y_clean, c=colors, cmap='rainbow', vmin=0., vmax=100., alpha=0.5)\n",
    "cb = plt.colorbar(sc, ax=axs[1])\n",
    "cb.set_label('Frequency (%s)' % '%')\n",
    "axs[1].set_title('Residue Plot without noise')\n",
    "\n",
    "plt.savefig('%s_residue_plot_combined.png' % args.out)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #====================================================================================================\n",
    "# # END\n",
    "# #===================================================================================================="
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
